---
title: "Inicio r√°pido"
linkTitle: "Inicio r√°pido"
weight: 1
description: "Build a working AI agent in 10 minutes. Start with a stub, add streaming, validation, then connect a real LLM."
llm_optimized: true
aliases:
---

{{< alert title="Tested Example" color="info" >}}
Este c√≥digo se prueba en CI. Si algo no funciona, [file an issue](https://github.com/goadesign/goa-ai/issues).
{{< /alert >}}

En los pr√≥ximos 10 minutos, construir√°s un sistema agentic listo para producci√≥n desde cero. Herramientas de tipo seguro, streaming en tiempo real, validaci√≥n autom√°tica con reintentos de autocuraci√≥n, integraci√≥n LLM y composici√≥n de agentes, todo desde un DSL declarativo. Cosas geniales.

**Qu√© construir√°s:**

1. **Agente Stub** - entender el bucle plan/ejecutar (3 min)
2. **Streaming** - ver los eventos a medida que ocurren
3. **Validaci√≥n** - reintento autom√°tico en entradas err√≥neas
4. **Real LLM** - conectar OpenAI o Claude
5. **Composici√≥n de agentes** - agentes que llaman a agentes

Al final, tendr√°s un agente de tipo seguro con herramientas validadas, streaming en tiempo real y la base para el despliegue en producci√≥n.

---

## Prerrequisitos

```bash
# Go 1.24+
go version

# Install Goa CLI
go install goa.design/goa/v3/cmd/goa@latest
```

---

## Paso 1: Configuraci√≥n del proyecto

```bash
mkdir quickstart && cd quickstart
go mod init quickstart
go get goa.design/goa/v3@latest goa.design/goa-ai@latest
```

Cree `design/design.go`. Este fichero define tu agente y sus herramientas usando el DSL de Goa. Piensa en √©l como un contrato: qu√© puede hacer el agente, qu√© entradas acepta y qu√© salidas devuelve.

```go
package design

import (
    . "goa.design/goa/v3/dsl"
    . "goa.design/goa-ai/dsl"
)

// Service groups related agents and methods
var _ = Service("demo", func() {
    // Agent defines an AI agent with a name and description
    Agent("assistant", "A helpful assistant", func() {
        // Use declares a toolset the agent can access
        Use("weather", func() {
            // Tool defines a capability the LLM can invoke
            Tool("get_weather", "Get current weather", func() {
                // Args defines the input schema (what the LLM sends)
                Args(func() {
                    Attribute("city", String, "City name")
                    Required("city")
                })
                // Return defines the output schema (what the tool returns)
                Return(func() {
                    Attribute("temperature", Int, "Temperature in Celsius")
                    Attribute("conditions", String, "Weather conditions")
                    Required("temperature", "conditions")
                })
            })
        })
    })
})
```

Generar c√≥digo:

```bash
goa gen quickstart/design
```

Esto crea un directorio `gen/` con:
- **Ayudantes de registro de agente** - cablea tu agente al tiempo de ejecuci√≥n
- **Especificaciones de herramientas y codecs** - manejo seguro de la carga √∫til/resultado
- **Esquemas JSON** - para definiciones de herramientas LLM

Nunca edites archivos en `gen/` - se regeneran en cada ejecuci√≥n de `goa gen`.

---

## Paso 2: Ejecutar con un Stub Planner

Antes de conectar un LLM real, vamos a entender c√≥mo funcionan los agentes de Goa-AI utilizando un planificador stub. Esto hace el flujo expl√≠cito y te ayuda a depurar problemas m√°s tarde.

**El bucle plan/execute:**
1. El tiempo de ejecuci√≥n llama a `PlanStart` con el mensaje del usuario
2. El planificador devuelve una respuesta final o la herramienta llama a
3. Si se ha llamado a herramientas, el tiempo de ejecuci√≥n las ejecuta y llama a `PlanResume` con los resultados
4. El bucle contin√∫a hasta que el planificador devuelve una respuesta final

Crea `main.go`:

```go
package main

import (
    "context"
    "fmt"

    // Generated package for our assistant agent
    assistant "quickstart/gen/demo/agents/assistant"
    "goa.design/goa-ai/runtime/agent/model"
    "goa.design/goa-ai/runtime/agent/planner"
    "goa.design/goa-ai/runtime/agent/runtime"
)

// StubPlanner implements the planner.Planner interface.
// A real planner would call an LLM; this one hardcodes the flow.
type StubPlanner struct{}

// PlanStart is called with the initial user message.
// Return ToolCalls to invoke tools, or FinalResponse to end the run.
func (p *StubPlanner) PlanStart(ctx context.Context, in *planner.PlanInput) (*planner.PlanResult, error) {
    // Request a tool call: "toolset.tool_name" format
    return &planner.PlanResult{
        ToolCalls: []*planner.ToolCall{{
            Name:    "weather.get_weather",        // toolset.tool format
            Payload: []byte(`{"city": "Tokyo"}`),  // JSON matching Args schema
        }},
    }, nil
}

// PlanResume is called after tools execute, with their results in in.Messages.
// Decide: call more tools, or return a final response.
func (p *StubPlanner) PlanResume(ctx context.Context, in *planner.PlanResumeInput) (*planner.PlanResult, error) {
    // We have tool results; return final answer
    return &planner.PlanResult{
        FinalResponse: &planner.FinalResponse{
            Message: &model.Message{
                Role:  model.ConversationRoleAssistant,
                Parts: []model.Part{model.TextPart{Text: "Tokyo is 22¬∞C and sunny!"}},
            },
        },
    }, nil
}

// StubExecutor implements runtime.Executor.
// Called when the planner requests a tool. Returns the tool's result.
type StubExecutor struct{}

func (e *StubExecutor) Execute(ctx context.Context, meta runtime.ToolCallMeta, req *planner.ToolRequest) (*planner.ToolResult, error) {
    // Return data matching the Return schema defined in the DSL
    return &planner.ToolResult{
        Name:   req.Name,
        Result: map[string]any{"temperature": 22, "conditions": "Sunny"},
    }, nil
}

func main() {
    ctx := context.Background()

    // Create runtime with in-memory engine (no external dependencies)
    rt := runtime.New()
    sessionID := "demo-session"
    if _, err := rt.CreateSession(ctx, sessionID); err != nil {
        panic(err)
    }

    // Register the agent with its planner and executor
    err := assistant.RegisterAssistantAgent(ctx, rt, assistant.AssistantAgentConfig{
        Planner:  &StubPlanner{},
        Executor: &StubExecutor{},
    })
    if err != nil {
        panic(err)
    }

    // Create a typed client for the agent
    client := assistant.NewClient(rt)

    // Start a run with a user message
    out, err := client.Run(ctx, sessionID, []*model.Message{{
        Role:  model.ConversationRoleUser,
        Parts: []model.Part{model.TextPart{Text: "What's the weather?"}},
    }})
    if err != nil {
        panic(err)
    }

    // Print the result
    fmt.Println("RunID:", out.RunID)
    if out.Final != nil {
        for _, p := range out.Final.Parts {
            if tp, ok := p.(model.TextPart); ok {
                fmt.Println("Assistant:", tp.Text)
            }
        }
    }
}
```

Ejecutar:

```bash
go mod tidy && go run main.go
```

Salida:
```
RunID: demo.assistant-abc123
Assistant: Tokyo is 22¬∞C and sunny!
```

**Qu√© ha pasado:**
1. El tiempo de ejecuci√≥n llam√≥ a `PlanStart` ‚Üí el planificador solicit√≥ la herramienta `get_weather`
2. El tiempo de ejecuci√≥n ejecut√≥ la herramienta a trav√©s de `StubExecutor`
3. El tiempo de ejecuci√≥n llam√≥ a `PlanResume` con los resultados de la herramienta ‚Üí el planificador devolvi√≥ la respuesta final

El planificador stub hardcodes este flujo, pero un planificador LLM real sigue el mismo patr√≥n-s√≥lo decide din√°micamente basado en la conversaci√≥n.

### Opcional: a√±adir Prompt Override Store

Si quieres overrides de prompts gestionados en runtime desde el primer dia, conecta un prompt store al crear el runtime:

```go
import (
    promptmongo "goa.design/goa-ai/features/prompt/mongo"
    clientmongo "goa.design/goa-ai/features/prompt/mongo/clients/mongo"
)

promptClient, _ := clientmongo.New(clientmongo.Options{
    Client:   mongoClient,
    Database: "quickstart",
})
promptStore, _ := promptmongo.NewStore(promptClient)

rt := runtime.New(
    runtime.WithPromptStore(promptStore),
)
```

Tambien puedes ejecutar sin prompt store; en ese caso, el runtime usa solo los prompt specs base.

---

## Paso 3: A√±adir Streaming

Los agentes pueden ser opacos. Los eventos de streaming permiten ver exactamente lo que est√° ocurriendo, lo que resulta √∫til para depurar y crear interfaces de usuario en tiempo real.

Goa-AI emite eventos tipados a lo largo de la ejecuci√≥n: `ToolStart`, `ToolEnd`, `Workflow` cambios de fase, `AssistantReply` chunks, y m√°s. Se consumen a trav√©s de una interfaz `Sink`.

Vea los eventos a medida que suceden:

```go
package main

import (
    "context"
    "fmt"

    assistant "quickstart/gen/demo/agents/assistant"
    "goa.design/goa-ai/runtime/agent/model"
    "goa.design/goa-ai/runtime/agent/planner"
    "goa.design/goa-ai/runtime/agent/runtime"
    "goa.design/goa-ai/runtime/agent/stream"
)

// Same stub planner as before
type StubPlanner struct{}

func (p *StubPlanner) PlanStart(ctx context.Context, in *planner.PlanInput) (*planner.PlanResult, error) {
    return &planner.PlanResult{
        ToolCalls: []*planner.ToolCall{{Name: "weather.get_weather", Payload: []byte(`{"city":"Tokyo"}`)}},
    }, nil
}

func (p *StubPlanner) PlanResume(ctx context.Context, in *planner.PlanResumeInput) (*planner.PlanResult, error) {
    return &planner.PlanResult{
        FinalResponse: &planner.FinalResponse{
            Message: &model.Message{
                Role:  model.ConversationRoleAssistant,
                Parts: []model.Part{model.TextPart{Text: "Tokyo is 22¬∞C and sunny!"}},
            },
        },
    }, nil
}

type StubExecutor struct{}

func (e *StubExecutor) Execute(ctx context.Context, meta runtime.ToolCallMeta, req *planner.ToolRequest) (*planner.ToolResult, error) {
    return &planner.ToolResult{Name: req.Name, Result: map[string]any{"temperature": 22, "conditions": "Sunny"}}, nil
}

// ConsoleSink implements stream.Sink to receive events.
// Events are typed‚Äîswitch on the concrete type to handle each kind.
type ConsoleSink struct{}

func (s *ConsoleSink) Send(ctx context.Context, event stream.Event) error {
    // Type switch on event to handle different event kinds
    switch e := event.(type) {
    case stream.ToolStart:
        fmt.Printf("üîß Tool: %s\n", e.Data.ToolName)
    case stream.ToolEnd:
        fmt.Printf("‚úÖ Done: %s\n", e.Data.ToolName)
    case stream.Workflow:
        fmt.Printf("üìã %s\n", e.Data.Phase)
    // Other events: AssistantReply, PlannerThought, UsageDelta, etc.
    }
    return nil
}

func (s *ConsoleSink) Close(ctx context.Context) error { return nil }

func main() {
    ctx := context.Background()

    // Pass the sink to the runtime‚Äîall events flow through it
    rt := runtime.New(runtime.WithStream(&ConsoleSink{}))
    sessionID := "demo-session"
    if _, err := rt.CreateSession(ctx, sessionID); err != nil {
        panic(err)
    }

    _ = assistant.RegisterAssistantAgent(ctx, rt, assistant.AssistantAgentConfig{
        Planner:  &StubPlanner{},
        Executor: &StubExecutor{},
    })

    client := assistant.NewClient(rt)
    out, _ := client.Run(ctx, sessionID, []*model.Message{{
        Role:  model.ConversationRoleUser,
        Parts: []model.Part{model.TextPart{Text: "What's the weather?"}},
    }})

    fmt.Println("\nRunID:", out.RunID)
}
```

Salida:
```
üìã started
üîß Tool: weather.get_weather
‚úÖ Done: weather.get_weather
üìã completed

RunID: demo.assistant-abc123
```

---

## Paso 4: A√±adir Validaci√≥n

Los LLMs cometen errores. Enviar√°n cadenas vac√≠as, valores de enum inv√°lidos o JSON malformado. Sin validaci√≥n, estos errores bloquean sus herramientas o producen resultados basura.

Goa-AI valida las cargas √∫tiles de las herramientas en el l√≠mite, antes de que se ejecute el ejecutor. Las llamadas no v√°lidas devuelven un `RetryHint` que el planificador puede utilizar para autocorregirse. Esto sucede de forma autom√°tica; s√≥lo tiene que definir las restricciones.

Actualice `design/design.go` con restricciones:

```go
package design

import (
    . "goa.design/goa/v3/dsl"
    . "goa.design/goa-ai/dsl"
)

var _ = Service("demo", func() {
    Agent("assistant", "A helpful assistant", func() {
        Use("weather", func() {
            Tool("get_weather", "Get current weather", func() {
                Args(func() {
                    // MinLength/MaxLength: string length constraints
                    Attribute("city", String, "City name", func() {
                        MinLength(2)   // Rejects "" or "X"
                        MaxLength(100) // Rejects very long strings
                    })
                    // Enum: only these values are valid
                    Attribute("units", String, "Temperature units", func() {
                        Enum("celsius", "fahrenheit") // Rejects "kelvin"
                    })
                    Required("city") // city must be present
                })
                Return(func() {
                    Attribute("temperature", Int, "Temperature")
                    Attribute("conditions", String, "Weather conditions")
                    Required("temperature", "conditions")
                })
            })
        })
    })
})
```

Regenerar:

```bash
goa gen quickstart/design
```

Ahora si un planificador env√≠a `{"city": ""}` o `{"units": "kelvin"}`:

1. **Rechazado** en el l√≠mite (antes de que se ejecute el ejecutor)
2. **RetryHint** devuelto con error de validaci√≥n
3. El planificador puede **autocorregir** y reintentar

Esto es lo que devuelve el tiempo de ejecuci√≥n cuando falla la validaci√≥n:

```go
// When the LLM sends invalid input like {"city": "", "units": "kelvin"}
// the runtime returns a ToolResult with RetryHint instead of calling your executor:
&planner.ToolResult{
    Name: "weather.get_weather",
    RetryHint: &planner.RetryHint{
        Message: `validation failed: city length must be >= 2; units must be one of ["celsius", "fahrenheit"]`,
    },
}

// The planner sees this error and can retry with corrected input.
// With real LLMs, this self-correction happens automatically‚Äî
// the model reads the error, understands what went wrong, and fixes it.
```

No se bloquea. No hay an√°lisis manual. El LLM ve un mensaje de error claro y lo soluciona en el siguiente intento.

---

## Paso 5: LLM real

Ahora vamos a sustituir el stub por un LLM real. El trabajo del planificador es:
1. Construir una petici√≥n con el historial de conversaciones y las herramientas disponibles
2. Enviarla al modelo
3. Interpretar la respuesta, ya sean llamadas a herramientas o una respuesta final

El tiempo de ejecuci√≥n se encarga de todo lo dem√°s: ejecuci√≥n de herramientas, validaci√≥n, reintentos y streaming.

Conectarse a OpenAI o Claude. Primero, crea un planificador real que utilice el cliente modelo:

```go
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "os"

    assistant "quickstart/gen/demo/agents/assistant"
    "goa.design/goa-ai/features/model/openai"
    "goa.design/goa-ai/runtime/agent/model"
    "goa.design/goa-ai/runtime/agent/planner"
    "goa.design/goa-ai/runtime/agent/runtime"
    "goa.design/goa-ai/runtime/agent/stream"
)

// RealPlanner calls an actual LLM instead of hardcoding responses.
// It retrieves the model client from the runtime by ID.
type RealPlanner struct {
    systemPrompt string
}

func (p *RealPlanner) PlanStart(ctx context.Context, in *planner.PlanInput) (*planner.PlanResult, error) {
    // Get the model client by the ID we registered it with
    client, ok := in.Agent.ModelClient("openai")
    if !ok {
        return nil, fmt.Errorf("no model client")
    }

    // Build messages: system prompt first, then user messages
    msgs := append([]*model.Message{{
        Role:  model.ConversationRoleSystem,
        Parts: []model.Part{model.TextPart{Text: p.systemPrompt}},
    }}, in.Messages...)

    // Call the LLM with messages and available tools
    // in.Tools contains the JSON schemas generated from your DSL
    resp, err := client.Complete(ctx, &model.Request{
        Messages: msgs,
        Tools:    in.Tools,
    })
    if err != nil {
        return nil, err
    }

    return interpretResponse(resp)
}

func (p *RealPlanner) PlanResume(ctx context.Context, in *planner.PlanResumeInput) (*planner.PlanResult, error) {
    client, ok := in.Agent.ModelClient("openai")
    if !ok {
        return nil, fmt.Errorf("no model client")
    }

    // in.Messages now includes tool results from the previous turn
    msgs := append([]*model.Message{{
        Role:  model.ConversationRoleSystem,
        Parts: []model.Part{model.TextPart{Text: p.systemPrompt}},
    }}, in.Messages...)

    resp, err := client.Complete(ctx, &model.Request{
        Messages: msgs,
        Tools:    in.Tools,
    })
    if err != nil {
        return nil, err
    }

    return interpretResponse(resp)
}

// interpretResponse converts the LLM response to a PlanResult.
// If the LLM requested tools, return ToolCalls. Otherwise, return FinalResponse.
func interpretResponse(resp *model.Response) (*planner.PlanResult, error) {
    if len(resp.Content) == 0 {
        return nil, fmt.Errorf("empty response")
    }

    msg := resp.Content[len(resp.Content)-1]
    var toolCalls []*planner.ToolCall

    // Check each part of the response for tool calls or text
    for _, part := range msg.Parts {
        switch p := part.(type) {
        case model.ToolUsePart:
            // LLM wants to call a tool‚Äîconvert to ToolCall
            payload, _ := json.Marshal(p.Input)
            toolCalls = append(toolCalls, &planner.ToolCall{
                Name:    p.Name,
                Payload: payload,
            })
        case model.TextPart:
            // Text response (used if no tool calls)
        }
    }

    // If tools were requested, return them for execution
    if len(toolCalls) > 0 {
        return &planner.PlanResult{ToolCalls: toolCalls}, nil
    }

    // No tools‚Äîthis is the final answer
    return &planner.PlanResult{
        FinalResponse: &planner.FinalResponse{Message: &msg},
    }, nil
}

type WeatherExecutor struct{}

func (e *WeatherExecutor) Execute(ctx context.Context, meta runtime.ToolCallMeta, req *planner.ToolRequest) (*planner.ToolResult, error) {
    // Real implementation would call a weather API here
    return &planner.ToolResult{
        Name:   req.Name,
        Result: map[string]any{"temperature": 22, "conditions": "Sunny"},
    }, nil
}

// ConsoleSink streams assistant text to the console in real-time
type ConsoleSink struct{}

func (s *ConsoleSink) Send(ctx context.Context, event stream.Event) error {
    switch e := event.(type) {
    case stream.ToolStart:
        fmt.Printf("üîß Tool: %s\n", e.Data.ToolName)
    case stream.AssistantReply:
        // Print text chunks as they arrive (streaming output)
        fmt.Print(e.Data.Text)
    }
    return nil
}
func (s *ConsoleSink) Close(ctx context.Context) error { return nil }

func main() {
    ctx := context.Background()

    // --- OpenAI ---
    modelClient, err := openai.NewFromAPIKey(os.Getenv("OPENAI_API_KEY"), "gpt-4o")
    if err != nil {
        panic(err)
    }

    // --- Claude via Bedrock (uncomment to use instead) ---
    // import "goa.design/goa-ai/features/model/bedrock"
    //
    // bedrockClient, err := bedrock.New(bedrock.Options{
    //     Region: "us-east-1",
    //     Model:  "anthropic.claude-sonnet-4-20250514-v1:0",
    // })
    // if err != nil {
    //     panic(err)
    // }
    // // Then use: runtime.WithModelClient("claude", bedrockClient)
    // // And in planner: in.Agent.ModelClient("claude")

    // Create runtime with streaming and model client
    // The ID ("openai") is how the planner retrieves it
    rt := runtime.New(
        runtime.WithStream(&ConsoleSink{}),
        runtime.WithModelClient("openai", modelClient),
    )
    sessionID := "demo-session"
    if _, err := rt.CreateSession(ctx, sessionID); err != nil {
        panic(err)
    }

    // Register the agent with the real planner
    err = assistant.RegisterAssistantAgent(ctx, rt, assistant.AssistantAgentConfig{
        Planner:  &RealPlanner{systemPrompt: "You are a helpful weather assistant."},
        Executor: &WeatherExecutor{},
    })
    if err != nil {
        panic(err)
    }

    // Run the agent
    client := assistant.NewClient(rt)
    out, err := client.Run(ctx, sessionID, []*model.Message{{
        Role:  model.ConversationRoleUser,
        Parts: []model.Part{model.TextPart{Text: "What's the weather in Paris?"}},
    }})
    if err != nil {
        panic(err)
    }

    fmt.Println("\n\nRunID:", out.RunID)
}
```

Ejec√∫talo con tu clave API:

```bash
export OPENAI_API_KEY="sk-..."
go run main.go
```

Todos los adaptadores de modelos implementan la misma interfaz `model.Client`, por lo que cambiar entre OpenAI, Claude u otros proveedores es solo un cambio de configuraci√≥n: tu c√≥digo de planificador sigue siendo el mismo.

---

## Paso 6: Composici√≥n del Agente

Los sistemas de IA del mundo real no son agentes individuales, sino especialistas que trabajan juntos. Un agente de investigaci√≥n recopila datos, un analista los interpreta y un escritor da formato al resultado.

Goa-AI soporta esto de forma nativa con **agente-como-herramienta**. Cualquier agente puede exponer capacidades que otros agentes invocan como herramientas. El agente anidado se ejecuta con su propio planificador y herramientas, pero dentro del flujo de trabajo del agente padre: transacci√≥n √∫nica, historial unificado, trazabilidad completa.

Los agentes pueden llamar a otros agentes como herramientas. A√±adir a `design/design.go`:

```go
package design

import (
    . "goa.design/goa/v3/dsl"
    . "goa.design/goa-ai/dsl"
)

// Weather specialist agent‚Äîhas its own tools and planner
var _ = Service("weather", func() {
    Agent("forecaster", "Weather specialist", func() {
        // Internal tools only this agent can use
        Use("weather_tools", func() {
            Tool("get_forecast", "Get forecast", func() {
                Args(func() {
                    Attribute("city", String, "City")
                    Required("city")
                })
                Return(func() {
                    Attribute("forecast", String, "Forecast")
                    Required("forecast")
                })
            })
        })

        // Export makes this agent callable as a tool by other agents.
        // The exported toolset defines the interface other agents see.
        Export("ask_weather", func() {
            Tool("ask", "Ask weather specialist", func() {
                Args(func() {
                    Attribute("question", String, "Question")
                    Required("question")
                })
                Return(func() {
                    Attribute("answer", String, "Answer")
                    Required("answer")
                })
            })
        })
    })
})

// Main assistant uses the weather agent as a tool
var _ = Service("demo", func() {
    Agent("assistant", "A helpful assistant", func() {
        // UseAgentToolset imports an exported toolset from another agent.
        // Args: service name, agent name, exported toolset name
        UseAgentToolset("weather", "forecaster", "ask_weather")
    })
})
```

Regenerar:

```bash
goa gen quickstart/design
```

Ahora cuando el asistente necesite informaci√≥n meteorol√≥gica:
1. El planificador del asistente decide llamar a `ask_weather`
2. El tiempo de ejecuci√≥n invoca al agente meteorol√≥gico como una ejecuci√≥n hija
3. El agente meteorol√≥gico ejecuta su propio bucle plan/execute con sus propias herramientas
4. El agente meteorol√≥gico devuelve su respuesta al padre
5. El planificador del asistente recibe el resultado y contin√∫a

**Cada agente tiene su propio planificador, herramientas y contexto. El tiempo de ejecuci√≥n se encarga de la orquestaci√≥n, y se obtiene una visibilidad completa de ambas ejecuciones a trav√©s de eventos de streaming.

---

## What You Built

‚úÖ **Agente tipificado** con herramientas validadas por esquema
‚úÖ **Streaming de eventos** para visibilidad en tiempo real
‚úÖ **Validaci√≥n** con sugerencias autom√°ticas de reintento
‚úÖ **Integraci√≥n de LLM** real
‚úÖ **Composici√≥n de agentes** con √°rboles de ejecuci√≥n

Todo ello desde un DSL declarativo. El dise√±o es su fuente de verdad: c√°mbielo, regen√©relo y sus tipos, esquemas y validaci√≥n se mantendr√°n sincronizados autom√°ticamente.

**Qu√© se ejecuta bajo el cap√≥:**
- Los c√≥decs generados manejan la serializaci√≥n JSON con los tipos adecuados
- La validaci√≥n se ejecuta en el l√≠mite antes de que su c√≥digo se ejecute
- El bucle plan/execute gestiona el estado y los reintentos
- Los eventos fluyen a cualquier sumidero que configure

Esta es la base. Para la producci√≥n, a√±adir√° Temporal para la durabilidad, Mongo para la persistencia y Pulse para la transmisi√≥n distribuida, pero el c√≥digo del agente seguir√° siendo el mismo.

---

## Pr√≥ximos pasos

| Gu√≠a | Lo que aprender√° |
|-------|-------------------|
| [DSL Reference](dsl-reference/) | Todas las funciones de DSL: pol√≠ticas, MCP, registros | | [Runtime](runtime/)
| [Runtime](runtime/) | Planificar/ejecutar bucle, motores, almacenes de memoria |
| [Conjuntos de herramientas](toolsets/) | Herramientas respaldadas por servicios, transformaciones, ejecutores | 
| [Composici√≥n de agentes](agent-composition/) | Profundizaci√≥n en patrones de agentes como herramientas |
| [Producci√≥n](production/) | Configuraci√≥n temporal, streaming a UIs, limitaci√≥n de tasa |
